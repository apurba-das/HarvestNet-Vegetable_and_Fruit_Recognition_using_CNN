{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L06-IjPTdlf",
        "outputId": "b3e8092a-0eb2-415b-ce24-e8be16fc80be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.7434 Acc: 0.4364\n",
            "validation Loss: 1.2471 Acc: 0.6567\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9755 Acc: 0.6828\n",
            "validation Loss: 0.7622 Acc: 0.7820\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.6773 Acc: 0.7830\n",
            "validation Loss: 0.4222 Acc: 0.8593\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.5066 Acc: 0.8389\n",
            "validation Loss: 0.1931 Acc: 0.9363\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.4238 Acc: 0.8651\n",
            "validation Loss: 0.3414 Acc: 0.8860\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.3480 Acc: 0.8927\n",
            "validation Loss: 0.2768 Acc: 0.9123\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.3064 Acc: 0.9045\n",
            "validation Loss: 0.2297 Acc: 0.9293\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.2770 Acc: 0.9161\n",
            "validation Loss: 0.1935 Acc: 0.9387\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.2385 Acc: 0.9253\n",
            "validation Loss: 0.0994 Acc: 0.9693\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.2253 Acc: 0.9272\n",
            "validation Loss: 0.0684 Acc: 0.9753\n",
            "Best validation Acc: 0.975333\n",
            "Test Accuracy: 0.9733\n",
            "Precision: 0.9742\n",
            "Recall: 0.9733\n",
            "F1 Score: 0.9734\n",
            "Mean Loss: 0.0687\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Specify data directory\n",
        "data_dir = '/content/drive/My Drive/Vegetable Images'\n",
        "\n",
        "# Constants'\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "# Load datasets with ImageFolder\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'validation', 'test']}\n",
        "\n",
        "# Define dataloaders\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'validation', 'test']}\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def setup_model():\n",
        "    model = models.resnet18(pretrained=False)  # Using ResNet without pretraining\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, len(image_datasets['train'].classes))\n",
        "    return model.to(device)\n",
        "\n",
        "def train_model(model, criterion, optimizer, num_epochs=10):\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'validation' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "\n",
        "    print('Best validation Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "# Usage\n",
        "model = setup_model()\n",
        "\n",
        "# Train model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n",
        "train_model(model, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    mean_loss = np.mean(losses)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print(f'Mean Loss: {mean_loss:.4f}')\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Evaluate the model\n",
        "y_true, y_pred = evaluate_model(model, dataloaders['test'])\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH4U0W3fe7y6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}